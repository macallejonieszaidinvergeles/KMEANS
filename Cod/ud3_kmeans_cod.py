# -*- coding: utf-8 -*-
"""UD3 kmeans COD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KYk3Yps5ULgI4FfqRAMWYlH5jbX_V5Q0

##Importación de librerías necesarias
"""

from google.colab import drive
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/UD3kmeans/cod.csv")

df

"""##Preprocesamiento y elección de variables relevantes para el estudio.

Filtrado de nulos
"""

df.isnull().sum()

"""Filtrado de que por lo menos haya jugado 1"""

df = df[df['timePlayed'] >= 1]

"""Conversion del dataframe to numpy array y solo nos quedamos con las columnas mas significativas"""

df.drop(["name","level","prestige","timePlayed","averageTime",], axis = 1, inplace=True)

df

"""son los campos mas significativos a la hora de valorar un jugador"""

df.columns

df_x = df.to_numpy()

X = df_x

# X

"""## Detección del número de clusters

Por tratarse de un caso real el número de clusters $K$ es desconocido. Vamos a aplicar  el criterio basado en el BIC para el cálculo de este hiperparámetro, para ello haremos uso de la función *BIC* (previamente empleada en el Ejemplo 6.1).  

Tras ejecutar el siguiente código podemos observar que el BIC decrece hasta $K=4$ y a partir de este, aumenta. Por lo tanto $K=4$ será el valor elegido.
"""

def BIC(K, grupos, X):
  """
  K: Número de grupos (clusters)
  grupos: Vector que contiene los grupos de los datos
  X: Matriz de datos
  """
  N = X.shape[0] # Número de datos
  P = X.shape[1] # Número de variables
  xi = np.zeros((1,K)) # Vector xi

  # Calculamos el sumario de xi en la fórmula
  for k in range(0, K):
    suma = 0
    for j in range(0, P):
      sigma = np.square(np.std(X[:, j]))
      sigma_j = np.square(np.std(X[grupos==k, j]))
      suma += 0.5*np.log(sigma + sigma_j)

    n_k = sum(grupos==k) # Número de elementos en el grupo k
    xi[0, k] = -n_k*suma

  bic = -2*np.sum(xi) + 2*K*P*np.log(N)
  return bic

## Calculo del BIC
Kmax = 10
BIC_array = []
for k in range(2, Kmax+1):
  kmeans = KMeans(n_clusters=k, n_init=15, random_state=100)
  grupos = kmeans.fit_predict(X)
  BIC_array.append(BIC(k, grupos, X))

# Dibujamos el BIC obtenido para cada valor de k
plt.figure(figsize=(9, 6))
plt.plot(np.arange(2, Kmax+1), BIC_array, "ko-")
plt.title("Valor del BIC en función de K", fontsize=16)
plt.xlabel("K", fontsize=14)
plt.ylabel("BIC(K)", fontsize=14)
plt.show()

"""## Detección de outliers (anomalías)

Vamos a aplicar el método de Jackknife para detectar las observaciones influyentes.  El siguiente código detecta los outliers y los elimina del análisis y los representa visualmente. Notar que si el número de réplicas en el número de veces que se realiza el algoritmo de las k-medias es pequeño el procedimiento detecta diferentes conjunto de outliers ya que llega a diferentes soluciones. Hemos fijado este valor a $30$.
"""

## Detección de outliers
N = X.shape[0] # Número de observaciones
K = 4 # Número de clusters el que se ve en el grafico

SSE = []
for i in range(0, N):
  X_sin_i = np.delete(X, i, axis=0) # Eliminamos la observación i
  # Aplicamos K-medias a X_sin_i y obtenemos el índice SSE
  kmeans = KMeans(n_clusters=K, n_init=30, random_state=100).fit(X_sin_i)
  SSE.append(kmeans.inertia_)

min(SSE)

max(SSE)

## Detección visual de outliers
# Dibujamos el SSE obtenido eliminando cada observación
plt.figure(figsize=(9, 6))
plt.plot(np.arange(0, N), SSE, "ko-")
plt.title("Valor del índice SSE eliminando el dato i", fontsize=16)
plt.xlabel("Dato i", fontsize=14)
plt.ylabel("SSE", fontsize=14)
plt.show()

## Detección analítica de outliers
sigma = np.std(SSE) # Desviación típica de SSE
mu = np.mean(SSE) # Media
umbral = 2 # Umbral: 2 para distribuciones normales y 3 para cualquier otra distribución

outliers = []
for i in range(0, N):
  if np.abs(SSE[i]-mu) > umbral*sigma:
    outliers.append(i)
print(outliers)

## Eliminación de los outliers
X_new = np.delete(X, outliers, axis=0)
X_new.shape

df = df.drop(outliers,axis=0)

"""## Determinación de patrones y estudio

Finalmente calculamos los patrones mediante el algoritmo $K-$means, los repesentamos gráficamente
"""

## Cálculo de patrones
kmeans = KMeans(n_clusters=K, n_init=30, random_state=100)
pred = kmeans.fit_predict(X_new)
centroides = kmeans.cluster_centers_

centroides

df["label"] = pred

df.head().T

#Scatterplot of the clusters
plt.figure(figsize=(10,6))
sns.scatterplot(x = 'kills',y = 'deaths',hue="label",palette=['green','orange','red','blue'], legend='full', data = df ,s = 60 )
plt.xlabel('kills')
plt.ylabel('deaths') 
plt.title('kills vs deaths')
plt.show()

N_patron = np.bincount(pred) # Contar el número veces del label que se repite 
tabla = pd.DataFrame(columns=["Número de grupo", "Número de observaciones del grupo","Centroide:característica wins",
                              "Centroide:característica kills", "Centroide:característica kdRatio","Centroide:característica killstreak",
                              "Centroide:característica losses", "Centroide:característica hits","Centroide:característica headshots",
                              "Centroide:característica gamesPlayed", "Centroide:característica assists","Centroide:característica misses",
                              "Centroide:característica xp", "Centroide:característica scorePerMinute","Centroide:característica shots",
                              "Centroide:característica deaths"  
                            ])

# Index(['wins', 'kills', 'kdRatio', 'killstreak', 'losses', 'hits', 'headshots',
#        'gamesPlayed', 'assists', 'misses', 'xp', 'scorePerMinute', 'shots',
#        'deaths'],


for k in range(K):
  tabla.loc[k, "Número de grupo"] = k
  tabla.loc[k, "Número de observaciones del grupo"] = N_patron[k]  
  tabla.loc[k, "Centroide:característica wins"] = centroides[k][0]
  tabla.loc[k, "Centroide:característica kills"] = centroides[k][1]
  tabla.loc[k, "Centroide:característica kdRatio"] = centroides[k][2]
  tabla.loc[k, "Centroide:característica killstreak"] = centroides[k][3]
  tabla.loc[k, "Centroide:característica losses"] = centroides[k][4]
  tabla.loc[k, "Centroide:característica hits"] = centroides[k][5]
  tabla.loc[k, "Centroide:característica headshots"] = centroides[k][6]
  tabla.loc[k, "Centroide:característica gamesPlayed"] = centroides[k][7]
  tabla.loc[k, "Centroide:característica assists"] = centroides[k][8]
  tabla.loc[k, "Centroide:característica misses"] = centroides[k][9]
  tabla.loc[k, "Centroide:característica xp"] = centroides[k][10]
  tabla.loc[k, "Centroide:característica scorePerMinute"] = centroides[k][11]
  tabla.loc[k, "Centroide:característica shots"] = centroides[k][12]
  tabla.loc[k, "Centroide:característica deaths"] = centroides[k][13]
    

# Mostrar la tabla
display(tabla)



fig = plt.figure(figsize=(20,10))
ax = fig.add_subplot(111, projection='3d')


ax.scatter(df.wins[df.label == 0], df["kdRatio"][df.label == 0], df["scorePerMinute"][df.label == 0], c='magenta', label='Grupo 0', s=60)
ax.scatter(df.wins[df.label == 1], df["kdRatio"][df.label == 1], df["scorePerMinute"][df.label == 1], c='red', label='Grupo 1', s=60)
ax.scatter(df.wins[df.label == 2], df["kdRatio"][df.label == 2], df["scorePerMinute"][df.label == 2], c='green', label='Grupo 2', s=60)
ax.scatter(df.wins[df.label == 3], df["kdRatio"][df.label == 3], df["scorePerMinute"][df.label == 3], c='black', label='Grupo 3', s=60)

ax.view_init(30, 185)

plt.xlabel("wins")
plt.ylabel("kdRatio")
ax.set_zlabel('scorePerMinute')

# produce a legend with the unique colors from the scatter
ax.legend()

plt.show()